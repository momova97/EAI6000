{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is assignment 6 by Mohammad Movahedi\n"
     ]
    }
   ],
   "source": [
    "print( \"this is assignment 6 by Mohammad Movahedi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to analyze the book \"Meditations\" by Marcus Aurelius\n",
    "\n",
    "# Reading a local text file\n",
    "with open('Meditations.txt', 'r', encoding='utf-8') as file:\n",
    "    meditations_text = file.read() # read the file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 87711\n",
      "Number of sentences: 3438\n",
      "Number of files: 1\n",
      "Custom File ID: meditations\n",
      "Raw text sample: The Project Gutenberg eBook of Meditations\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: Meditations\n",
      "\n",
      "\n",
      "Author: Emperor of Rome Marcus Aurelius\n",
      "\n",
      "Release date: June 1, 2001 [eBook #2680]\n",
      "                Most recently updated: March 9, 2021\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK MEDITATIONS ***\n",
      "\n",
      "\n",
      "\n",
      "MEDITATIONS\n",
      "\n",
      "By Marcus Aurelius\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "\n",
      "     NOTES\n",
      "\n",
      "     INTRODUCTION\n",
      "\n",
      "     FIRST BOOK\n",
      "\n",
      "     SECOND BOOK\n",
      "\n",
      "     THIRD BOOK\n",
      "\n",
      "     FOURTH BOOK\n",
      "\n",
      "     FIFTH BOOK\n",
      "\n",
      "     SIXTH BOOK\n",
      "\n",
      "     SEVENTH BOOK\n",
      "\n",
      "     EIGHTH BOOK\n",
      "\n",
      "     NINTH BOOK\n",
      "\n",
      "     TENTH BOOK\n",
      "\n",
      "     ELEVENTH BOO\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize , RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize words\n",
    "words = word_tokenize(meditations_text)\n",
    "\n",
    "# Get number of words\n",
    "num_words = len(words)\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = sent_tokenize(meditations_text)\n",
    "\n",
    "# For a single file, you can assign a custom ID\n",
    "file_id = 'meditations'\n",
    "\n",
    "# You have only one file\n",
    "num_files = 1\n",
    "\n",
    "# Raw text sample\n",
    "raw_text_sample = meditations_text.strip()[:1000]\n",
    "\n",
    "print(f\"Number of words: {num_words}\")\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(f\"Number of files: {num_files}\")\n",
    "print(f\"Custom File ID: {file_id}\")\n",
    "#print(f\"Raw text sample: {raw_text_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized line_no100: ['extravagancy', 'day', 'great', 'excitement', 'rome', 'strife']\n",
      "Word counts: Counter({'extravagancy': 1, 'day': 1, 'great': 1, 'excitement': 1, 'rome': 1, 'strife': 1})\n",
      "Term Frequency Matrix: [[1 1 1 1 1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "\n",
    "# Function for POS tag conversion\n",
    "def penn2morphy(penntag):\n",
    "    return {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'}.get(penntag[:2], 'n')\n",
    "\n",
    "# Function for text lemmatization\n",
    "def lemmatize_sent(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "# Combined stopwords\n",
    "stoplist_combined = set(stopwords.words('english')).union(punctuation)\n",
    "\n",
    "# Preprocess and lemmatize the 100th line\n",
    "line_no100 = meditations_text.split('\\n')[99]\n",
    "lemmatized_line_no100 = [word for word in lemmatize_sent(line_no100) if word not in stoplist_combined]\n",
    "\n",
    "# Count word occurrences and create term frequency matrix\n",
    "word_counts = Counter(lemmatized_line_no100)\n",
    "count_vect = CountVectorizer(analyzer=lemmatize_sent)\n",
    "term_freq_matrix = count_vect.fit_transform([line_no100])\n",
    "\n",
    "print(\"Lemmatized line_no100:\", lemmatized_line_no100)\n",
    "print(\"Word counts:\", word_counts)\n",
    "print(\"Term Frequency Matrix:\", term_freq_matrix.toarray())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Objective</h1>\n",
    "I'm aiming to build a Naive Bayes classifier that can categorize sections of the \"Meditations\" text into different philosophical themes, such as 'ethics,' 'self-control,' or 'duty.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the text cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Initialize stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Read and clean the text\n",
    "sections = meditations_text.split(\"BOOK\")[1:]\n",
    "cleaned_sections = [clean_text(section) for section in sections]\n",
    "\n",
    "# Tokenize, remove stop words and lemmatize\n",
    "tokenized_and_cleaned_sections = []\n",
    "for section in cleaned_sections:\n",
    "    tokens = word_tokenize(section)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatized_tokens = [wnl.lemmatize(word) for word in filtered_tokens]\n",
    "    tokenized_and_cleaned_sections.append(lemmatized_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.71      1.00      0.83         5\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.71         7\n",
      "   macro avg       0.24      0.33      0.28         7\n",
      "weighted avg       0.51      0.71      0.60         7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Convert the list of words for each section back to string format\n",
    "sections_as_str = [' '.join(section) for section in tokenized_and_cleaned_sections]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Limit the max_features in TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit and transform the cleaned sections\n",
    "X = vectorizer.fit_transform(sections_as_str)\n",
    "\n",
    "# Refit LDA model with new data\n",
    "best_lda_model.fit(X)\n",
    "\n",
    "# Get the new topic distribution\n",
    "topic_distribution = best_lda_model.transform(X)\n",
    "labels = [np.argmax(dist) for dist in topic_distribution]\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7142857142857143\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      1.00      0.83         5\n",
      "           2       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.71         7\n",
      "   macro avg       0.36      0.50      0.42         7\n",
      "weighted avg       0.51      0.71      0.60         7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "clf = SVC()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The SVM model achieved a 71.43% accuracy on a small test set of 7 samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
